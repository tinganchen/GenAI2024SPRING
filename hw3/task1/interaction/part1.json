{"chatbot": [["Generate a concise 200-word abstract of the key findings from the introduction of our recent research paper on topic of model compression and acceleration, highlighting the most significant novelties and contributions.\nConvolutional neural networks (CNNs) have been\ndemonstrated as effective models in computer vision tasks,\nsuch as image segmentation [2, 35] and object detection\n[16, 30, 34]. However, CNNs are suffered from large com-\nputation costs and memory storage when deployed on the\nresource-limited mobile devices [7]. Therefore, various\nmodel acceleration methods are proposed, including prun-\ning [20, 22, 27, 29], quantization [6, 42, 43] and structure\nsimplification [10, 45]. increasing attentions due to the effectiveness of acceleration\non inference by reducing the bit widths of model weights\nand activations.\n\nIn existing quantization research, quantization-aware\ntraining (QAT) learned quantization parameters, including\nclipping ranges and scale parameters, from the training data\nand applied them to the testing data [3, 8, 11, 28, 47, 48]. In\ncontrast, zero-shot quantization (ZSQ) adopted the concepts\nof knowledge distillation and employed the batch normal-\nization means and variances from the full-precision model,\nto learn a quantized model that can generate similar features\nto reduce the quantization error [5,9,19,31,44]. The learned\nbatch statistics are also utilized in testing. However, the pre-\nvious approaches ignored the difference between the train-\ning and testing data. As shown in Fig. 1 (a), the real-world\nimage data are usually collected under inconsistent quali-\nties, such as different colors, brightness, and rotations, lead-\ning to non-i.i.d. (independently and identically distributed)\ndata [21]. Accordingly, it may induce a large quantization\nerror when using the trained parameters in testing.\nTo address this issue, we propose AlignQ to align the\ndata into the same domain for quantization to minimize the\nquantization error (illustrated in Fig. 1 (b)). In this paper,\nour idea is to exploit the cumulative distribution function\n(CDF) as the alignment function since the CDF of an arbi-\ntrary continuous distribution follows the uniform distribu-\ntion [24] (demonstrated in Sec. 3.1). The uniform space\nis appropriate for uniform quantization that is hardware-\nfriendly with a few simple operations [3, 8, 28]. In addition,\nCDF retains the data order, i.e., larger values still exceed\nsmall values after the transformation.\nFurthermore, our theoretical analysis indicates that no-\ntable changes in data correlations after quantization induce\na larger quantization error. Therefore, as shown in Fig. 1 (c),\nwe aim to preserve the data correlations after the alignment-\nquantization process. We leverage the Alternating Direc-\ntion Method of Multipliers (ADMM) optimization to min-\nimize the differences to reduce the quantization error. To\nachieve the two goals in this paper, to minimize 1) the\nprediction loss of the quantized models and 2) the differ-\nences of data correlations before and after the alignment-\nquantization process, ADMM addresses this multi-goal op-\ntimization problem by dividing it into sub-problems and\nsolving them [4].\nTo verify that the proposed AlignQ can reduce the quan-\ntization error derived from the non-i.i.d. in training and test-\ning data, we compare with the state-of-the-art not only on\nthe benchmark datasets, including CIFAR-10 [25], SVHN\n[33], ImageNet [37], but also on domain shift benchmarks,\nincluding digits [12, 13, 26, 33] and Office-31 [38].\nThe contributions are summarized as follows:\n1. We make the first attempt to design a new quantization\nscheme, AlignQ, that aligns the non-i.i.d. data to be\ni.i.d. to minimize the quantization error.\n2. We prove that the changes in data correlation af-\nter quantization induce a large quantization error and\nthereby leverage the ADMM optimization procedure\nto minimize the differences of the data correlations be-\nfore and after quantization to reduce the error.\n3. We compare AlignQ with the state-of-the-art on\nbenchmarks and the domain shift datasets. Experimen-\ntal results show that AlignQ achieves significant per-\nformance improvements, especially at low bit widths", "Convolutional neural networks (CNNs) have shown effectiveness in computer vision tasks but suffer from high computation costs and storage on mobile devices. Model acceleration methods like pruning, quantization, and structure simplification have gained attention for reducing inference time by reducing bit widths. Existing quantization methods may lead to high quantization error due to inconsistencies in training and testing data. We propose AlignQ, a new quantization scheme, to minimize quantization error by aligning non-i.i.d. data to be i.i.d. using CDF alignment and ADMM optimization. Our contributions include designing AlignQ, proving the impact of data correlations on quantization error, and demonstrating the effectiveness of AlignQ on benchmark and domain shift datasets. Experimental results show significant performance improvements, especially at low bit widths. AlignQ offers a promising approach to effectively reduce quantization error in CNNs and improve performance on resource-limited devices."]], "article": "Convolutional neural networks (CNNs) have been\ndemonstrated as effective models in computer vision tasks,\nsuch as image segmentation [2, 35] and object detection\n[16, 30, 34]. However, CNNs are suffered from large com-\nputation costs and memory storage when deployed on the\nresource-limited mobile devices [7]. Therefore, various\nmodel acceleration methods are proposed, including prun-\ning [20, 22, 27, 29], quantization [6, 42, 43] and structure\nsimplification [10, 45]. increasing attentions due to the effectiveness of acceleration\non inference by reducing the bit widths of model weights\nand activations.\n\nIn existing quantization research, quantization-aware\ntraining (QAT) learned quantization parameters, including\nclipping ranges and scale parameters, from the training data\nand applied them to the testing data [3, 8, 11, 28, 47, 48]. In\ncontrast, zero-shot quantization (ZSQ) adopted the concepts\nof knowledge distillation and employed the batch normal-\nization means and variances from the full-precision model,\nto learn a quantized model that can generate similar features\nto reduce the quantization error [5,9,19,31,44]. The learned\nbatch statistics are also utilized in testing. However, the pre-\nvious approaches ignored the difference between the train-\ning and testing data. As shown in Fig. 1 (a), the real-world\nimage data are usually collected under inconsistent quali-\nties, such as different colors, brightness, and rotations, lead-\ning to non-i.i.d. (independently and identically distributed)\ndata [21]. Accordingly, it may induce a large quantization\nerror when using the trained parameters in testing.\nTo address this issue, we propose AlignQ to align the\ndata into the same domain for quantization to minimize the\nquantization error (illustrated in Fig. 1 (b)). In this paper,\nour idea is to exploit the cumulative distribution function\n(CDF) as the alignment function since the CDF of an arbi-\ntrary continuous distribution follows the uniform distribu-\ntion [24] (demonstrated in Sec. 3.1). The uniform space\nis appropriate for uniform quantization that is hardware-\nfriendly with a few simple operations [3, 8, 28]. In addition,\nCDF retains the data order, i.e., larger values still exceed\nsmall values after the transformation.\nFurthermore, our theoretical analysis indicates that no-\ntable changes in data correlations after quantization induce\na larger quantization error. Therefore, as shown in Fig. 1 (c),\nwe aim to preserve the data correlations after the alignment-\nquantization process. We leverage the Alternating Direc-\ntion Method of Multipliers (ADMM) optimization to min-\nimize the differences to reduce the quantization error. To\nachieve the two goals in this paper, to minimize 1) the\nprediction loss of the quantized models and 2) the differ-\nences of data correlations before and after the alignment-\nquantization process, ADMM addresses this multi-goal op-\ntimization problem by dividing it into sub-problems and\nsolving them [4].\nTo verify that the proposed AlignQ can reduce the quan-\ntization error derived from the non-i.i.d. in training and test-\ning data, we compare with the state-of-the-art not only on\nthe benchmark datasets, including CIFAR-10 [25], SVHN\n[33], ImageNet [37], but also on domain shift benchmarks,\nincluding digits [12, 13, 26, 33] and Office-31 [38].\nThe contributions are summarized as follows:\n1. We make the first attempt to design a new quantization\nscheme, AlignQ, that aligns the non-i.i.d. data to be\ni.i.d. to minimize the quantization error.\n2. We prove that the changes in data correlation af-\nter quantization induce a large quantization error and\nthereby leverage the ADMM optimization procedure\nto minimize the differences of the data correlations be-\nfore and after quantization to reduce the error.\n3. We compare AlignQ with the state-of-the-art on\nbenchmarks and the domain shift datasets. Experimen-\ntal results show that AlignQ achieves significant per-\nformance improvements, especially at low bit widths"}